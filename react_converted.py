# In[1]:
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableSequence, RunnableMap
from langchain_community.vectorstores import FAISS, DistanceStrategy
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
import pandas as pd
import os
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.schema.runnable import RunnableLambda
import time
def timed(name):
    def wrapper(fn):
        def inner(x):
            print(f"â±ï¸ [{name}] ì‹œì‘")
            start = time.time()
            result = fn(x)
            end = time.time()
            print(f"âœ… [{name}] ì™„ë£Œ - ì†Œìš” ì‹œê°„: {end - start:.2f}ì´ˆ")
            return result
        return inner
    return wrapper

# In[2]:
os.environ["OPENAI_API_KEY"] = "sk-"

# In[3]:
df = pd.read_csv('./all_origin_updated.csv', encoding='utf-8-sig')
# In[4]:
df.head()

# %% ìƒˆë¡œìš´ ICC ë°ì´í„°í”„ë ˆì„ ìƒì„±
icc_df = pd.DataFrame(columns=["ticket_id", "components", "before_change", "after_change", "ICC"])

# In[5]:
# í™˜ê²½ êµ¬ì„±
llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
    )

#%%
df
# %%In [6]:
# ë°ì´í„° ì¤€ë¹„ ë° ë²¡í„° DB êµ¬ì¶•
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document

# %%dropna() ì´í›„ì—ë„ ticket_idê°€ í•¨ê»˜ ìœ ì§€ë˜ë„ë¡
corpus_df = df[["ticket_id_hashed", "generated_summary"]].dropna(subset=["generated_summary"])

# %%ê° ë¬¸ì„œë¥¼ Document ê°ì²´ë¡œ ë³€í™˜í•  ë•Œ ticket_idë„ í•¨ê»˜ metadataë¡œ ì¶”ê°€
documents = [
    Document(
        page_content=row["generated_summary"],
        metadata={"ticket_id": row["ticket_id_hashed"], "doc_id": f"doc_{i}"}
    )
    for i, (_, row) in enumerate(corpus_df.iterrows())
]
## Build vector database manually using OpenAIEmbeddings and numpy
# vectordb = FAISS.from_documents(documents, OpenAIEmbeddings(), distance_strategy=DistanceStrategy.COSINE)

#%%
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# %%
vectordb = FAISS.from_documents(documents, OpenAIEmbeddings())

# In[6]:
# í”„ë¡¬í”„íŠ¸ ì •ì˜

### Step 1: ICC íŒë‹¨
icc_prompt = PromptTemplate.from_template("""<Role> ë‹¹ì‹ ì€ ì œí’ˆ í”¼ë“œë°±ì„ ì½ê³  ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

<Task>: ì œí’ˆ í”¼ë“œë°±ì´ ì‹ ê·œ ê¸°ëŠ¥ ìš”ì²­ì´ë‚˜ ê°œì„  ì œì•ˆì´ë©´ "Proposal"ë¡œ íŒë³„í•©ë‹ˆë‹¤.  
ê·¸ë ‡ì§€ ì•Šê³  í”¼ë“œë°± ë‚´ìš©ì´ ì•„ë˜ì˜ ìœ í˜•ì— í•´ë‹¹í•˜ë©´ ICCë¡œ íŒë³„í•©ë‹ˆë‹¤.  
ICCë€ Issues(ì˜¤ë¥˜, ê³ ì¥, ê²°í•¨), Complaints(ë¶ˆë§Œ, ë¶ˆí¸, ë¶ˆì¾Œ), Comments(ë‹¨ìˆœë¬¸ì˜, ì˜ê²¬)ë¡œ êµ¬ì„±ëœ ì¼ë°˜ì ì¸ ë¬¸ì œ ì œê¸°ë‚˜ ì˜ê²¬ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

íŒë³„ ê¸°ì¤€:
- í”¼ë“œë°±ì´ ì‹ ê·œ ê¸°ëŠ¥ ìš”ì²­ ë˜ëŠ” ê°œì„  ì œì•ˆì´ë©´: "Proposal"
- í”¼ë“œë°±ì— ì•„ë˜ì˜ ë‹¨ì–´ê°€ í¬í•¨ë˜ê±°ë‚˜ ê·¸ ì˜ë¯¸ì— í•´ë‹¹í•˜ë©´: "ICCë¡œ íŒë³„ëœ ê²ƒìœ¼ë¡œ ë³´ì—¬ì§‘ë‹ˆë‹¤"
  - ì˜¤ë¥˜
  - ê³ ì¥
  - ê²°í•¨
  - ë¶ˆë§Œ
  - ë¶ˆí¸
  - ë¶ˆì¾Œ
  - ë‹¨ìˆœë¬¸ì˜
  - ì˜ê²¬

ì¶œë ¥ í˜•ì‹:
- Proposalë¡œ íŒë³„ë˜ë©´: "Proposal"
- ICCë¡œ íŒë³„ë˜ë©´: "ICC"

ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶€ê°€ì ì¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

Input Summary:
{generated_summary}
""")


### Step 2: ì œì•ˆ ë¶„ë¦¬
split_prompt = PromptTemplate.from_template("""<Role3> You extract and organize multiple user proposals from text. </Role3>
<Task>: Your task is to separate each distinct proposal clearly. Identify and number each distinct proposal in the userâ€™s summarized text.

Reasoning: Use a Thought step to identify if the input contains more than one distinct idea (look for keywords like "and", punctuation, or multiple sentences indicating separate ideas). Then Act by listing each proposal separately in the required format. Keep each proposal phrased as a standalone improvement point.

Text:
{generated_summary}

<Format>: If there are multiple proposals in the input, output them as a JSON array of strings, where each string is one proposal. (e.g., ["Proposal 1", "Proposal 2"]). Ensure the JSON is valid. If there is only one proposal, still output a single-element JSON array with that proposal. Do not add any explanatory text, just the JSON.""")

### Step 3: ì²« ì œì•ˆ ì„ íƒ
first_prompt = PromptTemplate.from_template("""<Role4> You select the first proposal based on order of appearance. </Role4>
<Task>: Identify and return the first proposal in the list, exactly as it is, without any additional text.

Reasoning: Use a Thought step to parse the input list and find the first proposal in the original text. Then Act by outputting that first proposal verbatim. Do not include list markers, numbering, or any other proposals.

Original Text: 
{generated_summary}  

List of Proposals:
{proposals}

<Format>: Output the first proposal as a plain text string (no JSON, no list formatting, no quotes around it).""")

### Step 4: ì œì•ˆë¬¸ generated_summary
summary_prompt = PromptTemplate.from_template("""
<Role> You are a home appliance expert, specialized at summarizing electronic product suggestions. </Role>

<Task> Letâ€™s think step by step.

1. Understand the `{proposals}` thoroughly.

2. If the suggestion is too short to summarize or lacks context, instead of asking for more context, generate the summary with the original suggestion itself. You can also use the component `{components}`.

Strictly follow the format below. </Task>

<Format> Please follow these strict output rules:

- Return English text.
- The sentence should start with "The user suggests."
- Output must be in plain string format only.
- Do not include any extra explanations(such as Component:, only print the summary), metadata, notes, or formatting. </Format>
""")

### Step 5: ìœ ì‚¬ë„ ì •ë¦¬ (Not calling LLM)

### Step 6: Self-Consistency íŒë‹¨ (êµ¬ì¡°í™”ëœ í‰ê°€ ê¸°ì¤€)
sc_prompt = PromptTemplate.from_template("""
<Role6> ë‹¹ì‹ ì€ ì œì•ˆ ë¬¸ì¥ê³¼ ìœ ì‚¬ë„ ìƒìœ„ 10ê°œì˜ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë…ë¦½ì ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì„ ì´ 3íšŒ ë°˜ë³µí•˜ì—¬ Self-Consistencyë¥¼ í™•ë³´í•©ë‹ˆë‹¤. </Role6>

<Task>: ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”. ê·¸ë¦¬ê³  ì•„ë˜ í•­ëª©ë³„ ì ìˆ˜ë¥¼ ë°˜ë“œì‹œ ìˆ«ì(ì •ìˆ˜)ë¡œ í‰ê°€í•˜ê³ , ë§ˆì§€ë§‰ì— íŒë‹¨ ì´ì ê³¼ ì‚¬ê³  ì‹ ë¢°ë„ë„ ê³„ì‚°í•´ ì£¼ì„¸ìš”.

[ì…ë ¥ ì •ë³´]
Proposal:
{proposal_summary}

Top 10 Documents:
{top_10_table}

<í‰ê°€ í•­ëª©>: ì•„ë˜ ê° í•­ëª©ì— ëŒ€í•´ ì •ëŸ‰ ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.

- ì„ íƒ ë¬¸ì„œ (Tickect_id): [ì˜ˆ: fdff64d]
- ê¸°ëŠ¥ ë²”ì£¼ ì¼ì¹˜ì„± (Context Entity Recall): [0~25ì ]
- Claim ì»¤ë²„ë¦¬ì§€ (Context Recall): [0~25ì ]
- ê·¼ê±° ì¶©ì‹¤ë„ (Faithfulness): [0~25ì ]
- ì„¤ëª… íë¦„ ìœ ì‚¬ì„±: [0~25ì ]

<ì •ì„± í‰ê°€ í•­ëª©>: íŒë‹¨ ê³¼ì •ì„ ìŠ¤ìŠ¤ë¡œ í‰ê°€í•˜ì„¸ìš”.

- ê¸°ì¤€ ì ìš© ëª…í™•ì„±: [0~30ì ]
- ë…¼ë¦¬ì„±: [0~30ì ]
- ì„¤ëª…ì˜ ì„¤ë“ë ¥: [0~20ì ]
- ëª¨í˜¸ì„± ì—†ìŒ: [0~20ì ]

- í‰ê°€ ì´ì  í•©ê³„: [ìœ„ ë„¤ í•­ëª© í•©ê³„, 0~100ì ]
- ì‚¬ê³  ì‹ ë¢°ë„(Self-Eval): [ì •ì„± í‰ê°€ ì´ì ì— ë”°ë¼ ë°±ë¶„ìœ¨ ë³€í™˜]

<ì¶œë ¥ í˜•ì‹>: ì•„ë˜ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ë”°ë¥´ì„¸ìš”.

[1ë²ˆì§¸ ì‘ë‹µ]
- ì„ íƒ ë¬¸ì„œ: fdff61d
- ê¸°ëŠ¥ ë²”ì£¼ ì¼ì¹˜ì„± (Context Entity Recall): 25ì 
- Claim ì»¤ë²„ë¦¬ì§€: 20ì 
- ê·¼ê±° ì¶©ì‹¤ë„ (Faithfulness): 24ì 
- ì„¤ëª… íë¦„ ìœ ì‚¬ì„±: 22ì 
- í‰ê°€ ì´ì  í•©ê³„: 91ì 
- ê¸°ì¤€ ì ìš© ëª…í™•ì„±: 30ì 
- ë…¼ë¦¬ì„±: 28ì 
- ì„¤ëª…ì˜ ì„¤ë“ë ¥: 18ì 
- ëª¨í˜¸ì„± ì—†ìŒ: 20ì 
- ì‚¬ê³  ì‹ ë¢°ë„(Self-Eval): 96%
""")

### Step 7: ìµœì¢… íŒë‹¨
choose_prompt = PromptTemplate.from_template("""<Role7> ë‹¹ì‹ ì€ ë°˜ë³µëœ ì„¸ ë²ˆì˜ íŒë‹¨ ê²°ê³¼ë¥¼ í†µí•©í•˜ê³ , ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ìµœì¢… ì¶”ì²œ ë¬¸ì„œë¥¼ ê²°ì •í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. </Role7>

<Task>: ì„¸ ë²ˆì˜ Self-Consistency ê¸°ë°˜ íŒë‹¨ ì‘ë‹µê³¼ RAG ê¸°ë°˜ ìœ ì‚¬ë„ ì ìˆ˜, RAGAS ê¸°ë°˜ ì •ëŸ‰ ì§€í‘œë¥¼ í•¨ê»˜ ë¹„êµ ë¶„ì„í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ í•˜ë‚˜ ì„ ì •í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì •ë¦¬í•˜ì„¸ìš”.

Reasoning: ë‹¤ìŒ ì§€í‘œë“¤ì„ ëª¨ë‘ ìˆ˜ì§‘í•˜ì—¬ í†µí•© ì ìˆ˜ë¥¼ ê³„ì‚°í•œ ë’¤, ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”.

<í†µí•© í‰ê°€ ê¸°ì¤€>
1. Self-Consistency í‰ê·  ì‹ ë¢°ë„ (ê°€ì¤‘ì¹˜ 0.3)
2. ë™ì¼ ë¬¸ì„œ ë°˜ë³µ ì„ íƒ ì—¬ë¶€ (+10ì  ë³´ì •)
3. Self-Consistency íŒë‹¨ ì´ì  í‰ê·  (ê°€ì¤‘ì¹˜ 0.2)
4. RAG ê¸°ë°˜ Cosine ìœ ì‚¬ë„ (ê°€ì¤‘ì¹˜ 0.2)
5. Faithfulness (RAGAS ì§€í‘œ) (ê°€ì¤‘ì¹˜ 0.15)
6. Context Recall (RAGAS ì§€í‘œ) (ê°€ì¤‘ì¹˜ 0.15)

<ìµœì¢… ì„ íƒ ê¸°ì¤€>
- ì¢…í•© ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”.
- ë‹¨, ë™ì¼ ì ìˆ˜ì¼ ê²½ìš° Self-Consistencyì—ì„œ ë” ìì£¼ ì„ íƒëœ ë¬¸ì„œë¥¼ ìš°ì„  ê³ ë ¤í•˜ì„¸ìš”.
- ë¬¸ì„œ ìš”ì•½, ê¸°ëŠ¥ ëª©ì , ì„¤ëª… ë°©ì‹ì´ ì œì•ˆ ë¬¸ì¥ê³¼ ê°€ì¥ ìœ ì‚¬í•œì§€ë¥¼ ê·¼ê±°ë¡œ ì‚¼ì•„ì•¼ í•©ë‹ˆë‹¤.

Self-Consistency ì‘ë‹µ ëª©ë¡:
{self_consistency_responses}

RAG ìœ ì‚¬ë„ ëª©ë¡:
{top_10_table}

<Format>: ì•„ë˜ í˜•ì‹ì„ ë”°ë¼ ì¶œë ¥í•˜ì„¸ìš”.

> âœ… ìµœì¢… ì¶”ì²œ ë¬¸ì„œ: [ë¬¸ì„œì˜ Ticket ID (ì˜ˆ: d25d98b)]
> ğŸ“„ ì„ íƒëœ ë¬¸ì„œ ìš”ì•½: [ì„ íƒëœ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½]
> ğŸ”’ ì¶”ì²œ ì‹ ë¢°ë„ (%): [0~100 ì‚¬ì´ ìˆ˜ì¹˜, ë°˜ë³µ ì‘ë‹µê³¼ ì‹ ë¢°ë„ í‰ê· ì— ê¸°ë°˜]
> ğŸ“Š RAG ê¸°ë°˜ ìœ ì‚¬ë„ (%): [ì„ íƒëœ ë¬¸ì„œì˜ RAG ìœ ì‚¬ë„, ì—†ìœ¼ë©´ "N/A"]
> ğŸ§  ì„ íƒ ê·¼ê±° ìš”ì•½:
> - Self-Consistency ì‘ë‹µ ì¤‘ ë™ì¼ ë¬¸ì„œ ë°˜ë³µ ì„ íƒë¨ (+10ì )
> - í‰ê·  ì‹ ë¢°ë„ ë° í‰ê°€ ì´ì ì´ ê°€ì¥ ë†’ìŒ
> - RAG ìœ ì‚¬ë„ ë° Faithfulness/Context Recall ì ìˆ˜ ë˜í•œ ìš°ìˆ˜
> - ì œì•ˆ ë¬¸ì¥ê³¼ì˜ ê¸°ëŠ¥ ëª©ì  ë° ì„œìˆ  ë°©ì‹ì´ ê°€ì¥ ìœ ì‚¬í•¨
""")


# In[10]: inputs ì •ì˜

# inputs = {
#     "components": "ë¡œë´‡ì²­ì†Œê¸°",
#     "generated_summary": "ThinkQ í‰ë©´ë„ìƒì— ì„ ì„ ê·¸ì–´ ì²­ì†Œêµ¬ì—­ì„ ì§€ì •í•˜ë„ë¡ í•´ì£¼ì„¸ìš”"
# }

# In[11]:
from langchain.schema.runnable import RunnableSequence, RunnableMap, RunnableLambda
import json
import re

def calculate_cosine_similarity(query, documents, embedding_model):
    index = vectordb.index
    stored_vectors = index.reconstruct_n(0, index.ntotal)
    stored_vectors_np = np.array(stored_vectors)

    query_vector = embedding_model.embed_query(query)
    query_vector_np = np.array(query_vector).reshape(1, -1)

    similarities = cosine_similarity(query_vector_np, stored_vectors_np)[0]
    return similarities

def retrieve_context(proposal):
    embedding_model = OpenAIEmbeddings()
    similarities = calculate_cosine_similarity(proposal, documents, embedding_model)
    top_k_indices = similarities.argsort()[::-1][:10]

    rows = []
    docs = []
    result_records = []
    for idx, i in enumerate(top_k_indices, start=1):
        doc = documents[i]
        score = similarities[i]
        ticket_id = doc.metadata.get("ticket_id", "N/A")
        summary = doc.page_content.strip()[:100]
        rows.append(f"| {idx} | {ticket_id} | {score:.4f} | {summary} | |")
        result_records.append({
            "ìˆœìœ„": idx,
            "ticket_id": ticket_id,
            "Cosine ìœ ì‚¬ë„": round(score, 4),
            "ë¬¸ì„œ ìš”ì•½": summary
        })
        docs.append(doc)

    table = "\n".join(["| ìˆœìœ„ | Ticket ID | Cosine ìœ ì‚¬ë„ | ë¬¸ì„œ ìš”ì•½ | ì£¼ìš” í‚¤ì›Œë“œ |", "| --- | --- | --- | --- | --- |"] + rows)
    return table, docs, result_records

# Helper: format top-10 Document objects as markdown table for SC prompt
def format_top10_for_prompt(docs):
    rows = ["| ìˆœìœ„ | Ticket ID | Cosine ìœ ì‚¬ë„ | ë¬¸ì„œ ìš”ì•½ | ì£¼ìš” í‚¤ì›Œë“œ |", "| --- | --- | --- | --- | --- |"]
    for idx, doc in enumerate(docs, start=1):
        rows.append(f"| {idx} | {doc.metadata.get('ticket_id', 'N/A')} | N/A | {doc.page_content.strip()[:30]}... | |")
    return "\n".join(rows)

def parse_self_consistency_response(text):
    result = {}

    # ì„ íƒ ë¬¸ì„œ
    match_doc = re.search(r"- ì„ íƒ ë¬¸ì„œ:\s*(\S+)", text)
    result["ì„ íƒ ë¬¸ì„œ"] = match_doc.group(1) if match_doc else ""

    # ê¸°ëŠ¥ ë²”ì£¼ ì¼ì¹˜ì„± (Context Entity Recall)
    match_func = re.search(r"- ê¸°ëŠ¥ ë²”ì£¼ ì¼ì¹˜ì„±.*?:\s*(\d+)ì ", text)
    result["ê¸°ëŠ¥ ë²”ì£¼ ì¼ì¹˜ì„± (Context Entity Recall)"] = int(match_func.group(1)) if match_func else 0

    # Claim ì»¤ë²„ë¦¬ì§€
    match_claim = re.search(r"- Claim ì»¤ë²„ë¦¬ì§€:\s*(\d+)ì ", text)
    result["Claim ì»¤ë²„ë¦¬ì§€"] = int(match_claim.group(1)) if match_claim else 0

    # ê·¼ê±° ì¶©ì‹¤ë„ (Faithfulness)
    match_evidence = re.search(r"- ê·¼ê±° ì¶©ì‹¤ë„.*?:\s*(\d+)ì ", text)
    result["ê·¼ê±° ì¶©ì‹¤ë„ (Faithfulness)"] = int(match_evidence.group(1)) if match_evidence else 0

    # ì„¤ëª… íë¦„ ìœ ì‚¬ì„±
    match_flow = re.search(r"- ì„¤ëª… íë¦„ ìœ ì‚¬ì„±:\s*(\d+)ì ", text)
    result["ì„¤ëª… íë¦„ ìœ ì‚¬ì„±"] = int(match_flow.group(1)) if match_flow else 0

    # í‰ê°€ ì´ì  í•©ê³„
    match_total = re.search(r"- í‰ê°€ ì´ì  í•©ê³„:\s*(\d+)ì ", text)
    result["í‰ê°€ ì´ì  í•©ê³„"] = int(match_total.group(1)) if match_total else 0

    # ê¸°ì¤€ ì ìš© ëª…í™•ì„±
    match_criteria = re.search(r"- ê¸°ì¤€ ì ìš© ëª…í™•ì„±:\s*(\d+)ì ", text)
    result["ê¸°ì¤€ ì ìš© ëª…í™•ì„±"] = int(match_criteria.group(1)) if match_criteria else 0

    # ë…¼ë¦¬ì„±
    match_logic = re.search(r"- ë…¼ë¦¬ì„±:\s*(\d+)ì ", text)
    result["ë…¼ë¦¬ì„±"] = int(match_logic.group(1)) if match_logic else 0

    # ì„¤ëª…ì˜ ì„¤ë“ë ¥
    match_persuasion = re.search(r"- ì„¤ëª…ì˜ ì„¤ë“ë ¥:\s*(\d+)ì ", text)
    result["ì„¤ëª…ì˜ ì„¤ë“ë ¥"] = int(match_persuasion.group(1)) if match_persuasion else 0

    # ëª¨í˜¸ì„± ì—†ìŒ
    match_clarity = re.search(r"- ëª¨í˜¸ì„± ì—†ìŒ:\s*(\d+)ì ", text)
    result["ëª¨í˜¸ì„± ì—†ìŒ"] = int(match_clarity.group(1)) if match_clarity else 0

    # ì‚¬ê³  ì‹ ë¢°ë„ (Self-Eval)
    match_self_eval = re.search(r"- ì‚¬ê³  ì‹ ë¢°ë„\(Self-Eval\):\s*(\d+)%", text)
    result["ì‚¬ê³  ì‹ ë¢°ë„"] = int(match_self_eval.group(1)) if match_self_eval else 0

    return result

def format_self_consistency_table(responses):
    headers = ["í‰ê°€ í•­ëª©", "1ë²ˆì§¸ ì‘ë‹µ", "2ë²ˆì§¸ ì‘ë‹µ", "3ë²ˆì§¸ ì‘ë‹µ"]
    rows = [
        ["ì„ íƒ ë¬¸ì„œ"] + [resp.get("ì„ íƒ ë¬¸ì„œ", "") for resp in responses],
        ["ê¸°ëŠ¥ ë²”ì£¼ ì¼ì¹˜ì„± (Context Entity Recall)"] + [resp.get("ê¸°ëŠ¥ ë²”ì£¼ ì¼ì¹˜ì„± (Context Entity Recall)", 0) for resp in responses],
        ["Claim ì»¤ë²„ë¦¬ì§€"] + [resp.get("Claim ì»¤ë²„ë¦¬ì§€", 0) for resp in responses],
        ["ê·¼ê±° ì¶©ì‹¤ë„ (Faithfulness)"] + [resp.get("ê·¼ê±° ì¶©ì‹¤ë„ (Faithfulness)", 0) for resp in responses],
        ["ì„¤ëª… íë¦„ ìœ ì‚¬ì„±"] + [resp.get("ì„¤ëª… íë¦„ ìœ ì‚¬ì„±", 0) for resp in responses],
        ["í‰ê°€ ì´ì  í•©ê³„"] + [resp.get("í‰ê°€ ì´ì  í•©ê³„", 0) for resp in responses],
        ["ê¸°ì¤€ ì ìš© ëª…í™•ì„±"] + [resp.get("ê¸°ì¤€ ì ìš© ëª…í™•ì„±", 0) for resp in responses],
        ["ë…¼ë¦¬ì„±"] + [resp.get("ë…¼ë¦¬ì„±", 0) for resp in responses],
        ["ì„¤ëª…ì˜ ì„¤ë“ë ¥"] + [resp.get("ì„¤ëª…ì˜ ì„¤ë“ë ¥", 0) for resp in responses],
        ["ëª¨í˜¸ì„± ì—†ìŒ"] + [resp.get("ëª¨í˜¸ì„± ì—†ìŒ", 0) for resp in responses],
        ["ì‚¬ê³  ì‹ ë¢°ë„"] + [resp.get("ì‚¬ê³  ì‹ ë¢°ë„", 0) for resp in responses],
    ]

    # Build markdown table string
    header_line = "| " + " | ".join(headers) + " |"
    separator_line = "| " + " | ".join(["---"] * len(headers)) + " |"
    row_lines = []
    for row in rows:
        row_lines.append("| " + " | ".join(str(cell) for cell in row) + " |")
    return "\n".join([header_line, separator_line] + row_lines)

def prepare_self_consistency_input(x):
    return {
        "proposal": x["first_proposal"],
        "top_10_table": x["top_10_table"]
    }
load_context = RunnableLambda(lambda _: {
    "components": inputs["components"],
    "generated_summary": inputs["generated_summary"]
})

def update_icc_df(x):
    if x.get("icc_check") != "ICC":
        return x  # Proposalì´ë©´ ì•„ë¬´ ê²ƒë„ ì•ˆ í•˜ê³  ë°˜í™˜

    # ticket_idì™€ before/after_changeë¥¼ dfì—ì„œ ì°¾ì•„ì˜¤ê¸°
    ticket_row = df[df["generated_summary"] == x.get("generated_summary")]
    if not ticket_row.empty:
        ticket_id = ticket_row.iloc[0].get("ticket_id_hashed", "")
        before_change = ticket_row.iloc[0].get("before_change", "")
        after_change = ticket_row.iloc[0].get("after_change", "")
    else:
        ticket_id = ""
        before_change = ""
        after_change = ""

    new_row = {
        "ticket_id": ticket_id,
        "components": x.get("components", ""),
        "before_change": before_change,
        "after_change": after_change,
        "ICC": "ICC"
    }
    global icc_df
    icc_df = pd.concat([icc_df, pd.DataFrame([new_row])], ignore_index=True)

    return x

chain = RunnableSequence(
    RunnableLambda(lambda x: x),  # ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ í˜ë¦¼

    # Step 1: ICC íŒë‹¨ (timed)
    RunnableMap({
        "icc_check": RunnableLambda(
            timed("ICC íŒë‹¨")(
                lambda x: (icc_prompt | llm).invoke({"generated_summary": x["generated_summary"]}).content
            )
        ),
        "components": lambda x: x["components"],
        "generated_summary": lambda x: x["generated_summary"],
    }),

    # ICC ë¼ë²¨ì´ë©´ ì €ì¥ í›„ ì¢…ë£Œ, Proposalì€ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
    RunnableLambda(lambda x: (
        (update_icc_df(x) or print("ğŸ‘‰ íŒë³„ ê²°ê³¼ : ICC\n <ì¢…ë£Œ>") or exit(0))
    ) if x["icc_check"] == "ICC" else (
        (print("ğŸ‘‰ íŒë³„ ê²°ê³¼ : Proposal") or x)
    )),

    # Step 2: ì œì•ˆ ë¶„ë¦¬
    RunnableMap({
        "proposals": RunnableLambda(
            timed("ì œì•ˆ ë¶„ë¦¬")(
                lambda x: (split_prompt | llm).invoke({"generated_summary": x["generated_summary"]}).content
            )
        ),
        "components": lambda x: x["components"],
        "generated_summary": lambda x: x["generated_summary"],
    }),

    # Step 3: ì²« ì œì•ˆ ì„ íƒ
    RunnableMap({
        "first_proposal": RunnableLambda(
            timed("ì²« ì œì•ˆ ì„ íƒ")(
                lambda x: (first_prompt | llm).invoke({
                    "generated_summary": x["generated_summary"],
                    "proposals": json.dumps(x["proposals"])
                }).content
            )
        ),
        "generated_summary": lambda x: x["generated_summary"],
        "proposals": lambda x: x["proposals"].content if hasattr(x["proposals"], "content") else x["proposals"],
        "components": lambda x: x["components"],
    }),

    # Step 4: ì œì•ˆë¬¸ ìš”ì•½
    RunnableMap({
        "proposal_summary": RunnableLambda(
            timed("ì œì•ˆë¬¸ ìš”ì•½")(
                lambda x: llm.invoke(summary_prompt.format(
                    proposals=x["first_proposal"],
                    components=x["components"]
                )).content
            )
        ),
        "first_proposal": lambda x: x["first_proposal"],
        "components": lambda x: x["components"],
        "generated_summary": lambda x: x["generated_summary"],
    }),

    # Step 5: ìœ ì‚¬ë„ Top-10 ê²€ìƒ‰
    RunnableMap({
        "top_10_table": RunnableLambda(
            timed("ìœ ì‚¬ë„ Top-10 ê²€ìƒ‰")(
                lambda x: retrieve_context(x["proposal_summary"])[0]
            )
        ),
        "top_10_docs": lambda x: retrieve_context(x["proposal_summary"])[1],
        "top_10_records": lambda x: retrieve_context(x["proposal_summary"])[2],
        "first_proposal": lambda x: x["first_proposal"],
        "proposal_summary": lambda x: x["proposal_summary"],
        "components": lambda x: x["components"],
    }),

    # Step 6: Self-consistency íŒë‹¨ 3íšŒ
    RunnableMap({
        "self_consistency_responses": RunnableLambda(
            timed("Self-Consistency íŒë‹¨ (3íšŒ ë°˜ë³µ)")(
                lambda x: [
                    llm.invoke(sc_prompt.format(
                        proposal_summary=x["proposal_summary"],
                        top_10_table=format_top10_for_prompt(x["top_10_docs"])
                    )) for _ in range(3)
                ]
            )
        ),
        "first_proposal": lambda x: x["first_proposal"],
        "top_10_table": lambda x: x["top_10_table"],
        "top_10_docs": lambda x: x["top_10_docs"],
        "top_10_records": lambda x: x["top_10_records"],
        "proposal_summary": lambda x: x["proposal_summary"],
        "components": lambda x: x["components"],
    }),

    # Parse and format self_consistency_responses
    RunnableMap({
        "parsed_self_consistency": RunnableLambda(lambda x: [
            parse_self_consistency_response(resp.content if hasattr(resp, "content") else str(resp))
            for resp in x["self_consistency_responses"]
        ]),
        "self_consistency_responses": lambda x: x["self_consistency_responses"],
        "top_10_table": lambda x: x["top_10_table"],
        "first_proposal": lambda x: x["first_proposal"],
        "top_10_docs": lambda x: x["top_10_docs"],
        "top_10_records": lambda x: x["top_10_records"],
        "components": lambda x: x["components"],
    }),

    RunnableMap({
        "self_consistency_table": RunnableLambda(lambda x: format_self_consistency_table(x["parsed_self_consistency"])),
        "self_consistency_responses": lambda x: x["self_consistency_responses"],
        "top_10_table": lambda x: x["top_10_table"],
        "first_proposal": lambda x: x["first_proposal"],
        "top_10_docs": lambda x: x["top_10_docs"],
        "top_10_records": lambda x: x["top_10_records"],
        "components": lambda x: x["components"],
    }),

    # Step 7: ìµœì¢… íŒë‹¨
    RunnableMap({
        "final_result": RunnableLambda(
            timed("ìµœì¢… íŒë‹¨")(
                lambda x: (choose_prompt | llm).invoke({
                    "self_consistency_responses": json.dumps(
                        [resp.content if hasattr(resp, "content") else str(resp) for resp in x["self_consistency_responses"]]
                    ),
                    "top_10_table": x["top_10_table"]
                }).content
            )
        ),
        "self_consistency_responses": lambda x: x["self_consistency_responses"],
        "top_10_table": lambda x: x["top_10_table"],
        "self_consistency_table": lambda x: x["self_consistency_table"],
        "top_10_records": lambda x: x["top_10_records"],
        "components": lambda x: x["components"],
    })
)

# ëª¨ë“ˆí™”: main() í•¨ìˆ˜ ì •ì˜ ë° ë°˜í™˜ê°’ êµ¬ì„±
def main(inputs):
    print("\nğŸ“¥ ì…ë ¥ ì œì•ˆë¬¸:")
    print(inputs["generated_summary"])

    result = chain.invoke(inputs)
    print(result["final_result"])

    # RAGAS ì ìˆ˜ ì¶”ì¶œ
    import re
    final_text = result["final_result"]
    match_ragas = re.search(r"ğŸ“Š RAG ê¸°ë°˜ ìœ ì‚¬ë„.*?:\s*(\d+)", final_text)
    ragas_score = int(match_ragas.group(1)) if match_ragas else 0

    # ì„ íƒëœ ë¬¸ì„œì˜ ticket_id ì¶”ì¶œ
    match_ticket = re.search(r"âœ… ìµœì¢… ì¶”ì²œ ë¬¸ì„œ: \[(.*?)\]", final_text)
    ticket_id = match_ticket.group(1) if match_ticket else ""

    selected_row = df[df["ticket_id_hashed"] == ticket_id]
    if not selected_row.empty:
        keyword = selected_row.iloc[0].get("keyword", "N/A")
        before_change = selected_row.iloc[0].get("before_change", "")
        after_change = selected_row.iloc[0].get("after_change", "")
    else:
        keyword = "N/A"
        before_change = ""
        after_change = ""

    components = inputs["components"]
    generated_summary = inputs["generated_summary"]

    return ticket_id, components, before_change, after_change, generated_summary, ragas_score, keyword

if __name__ == "__main__":
    main()
